# 第一讲-神经网络
## 什么是神经网络
自然的神经元组成的网络。

对神经元建模：

多个加权输入，一个判断门限，一个输出。
## 什么是人工神经网络
### 人工神经元
建立人工神经元函数 function

<pre>function(bias + &Sigma;(weight<sub>i</sub> * input<sub>i</sub>))</pre>

如果 function 的值达到门限，输出 `output`。
### 建立人工神经网络
将神经元输出个数增加，增加后称为一层（Layer）。
将多层级联排列，就得到人工神经网络。

## 人工神经网络举例
### 专家系统
输入一张手写体阿拉伯数字的灰度图片，例如 18x18 的尺寸。

将其向量化为 324 维的向量，输入手写体数字辨识网络。

则输入神经元有 126 个，输出神经元有 0-9 共 10 个。

人为配置各个权重的值，就形成一个手写体阿拉伯数字图片，到数字的专家系统。

### 机器学习
机器学习的任务是学习权重值 `weight` 的值。

由于 `weight` 的数量巨大，

人为设定是很困难的，需要借助机器学习。

本质上机器学习的任务，就是*函数拟合* ，

从原始的数据出发，*逼近* 我们所需的函数。

可供参考的资料：

1. [TensorFlow 玩具程序](playground.tensorflow.org)

2. [Neural Networks and Deep Learning.com](neuralnetworksanddeeplearning.com)

## 构造神经网络的范例：函数拟合
### 目标
构造神经网络，拟合一个形状不规则的一元函数 y=f(x)
### 激活函数
#### Sigmoid 函数
sigmoid(x) = 1/(1+exp(x))

在 x 的绝对值很大的情况下，函数值趋向于 0 和 1。

但是在 x 的绝对值很小的情况下，函数值的变化比较剧烈。
#### 利用 Sigmoid 函数构造实用的激活函数
令

`modifiedx = weight * x + bias`

然后构造 sigmoid(modifiedx) 函数作为激活函数。

weight 绝对值越大，sigmoid 函数越陡峭，对输入值越敏感。

weight 的符号决定函数的变化方向，正的表示增函数，负的表示减函数。

bias 决定 sigmoid 函数的中心点，

中心点的坐标为(-bias/weight,0.5)。

规定变量

`s = -bias/weight`

则 s 可以唯一描述 sigmoid 函数。

#### 用激活函数逼近任意函数
用一增一减两个激活函数构造冲激函数；

用足够多的冲激函数来逼近原函数。

### 采用激活函数法的目的
1. 在拟合简单函数的情况下，使用级数展开法

2. 采用这种方法可以减少使用的神经元数量，比较节省能量。

3. 激活函数的取值自 0 到 1，表现为一个概率密度函数。它的输出可以看做一个概率。

4. 实际的神经网络当中，采用别的函数作为激活函数，

sigmoid 函数一般用在最后一层，即输出神经元的输出函数。

## 多元激活函数
### 多元 Sigmoid 函数
以二元 sigmoid 函数为例，

`output = sigmoid(modifiedx)+sigmoid(modifiedy)`

构造时只构造单变量的 sigmoid 函数，然后予以求和，避免构造二元函数。

最终得到的 output 是一元 sigmoid 函数的求和。

对 x 或对 y 构造一个冲激，会形成一个平面冲激（“墙”），

而将对 x 与对 y 的平面冲激，会形成一个冲激点（“塔”）。

将冲击点组合起来，就可以你和任意的二维曲面 F(x,y,z)=0

构造函数

`Trim = sigmoid(modifiedx) + Gate`

可以用于过滤阈值。

## 卷积神经网络（Convolutional Neural Network, CNN）
提出 CNN 的目的是简化神经网络，对 `weight` 向量进行降维。

### 卷积神经网络的应用场景
1. 图像分类；
2. 目标检测；
3. 图像边缘检测；
4. 语义分割：把图片上不同区域，按照其所指的物体分开；
5. 看图说话：一句话描述图片的内容；
6. 回答问题：输入图像和对图片内容的提问，输出答案。
7. 无人驾驶 - 需要应用到以上的全部技术。
### 卷积神经网络应用的本质
从无意义的 0,1 序列当中，像人类一样提取有价值的信息。
### 卷积神经网络的结构
卷积层 - 池化层 -卷积层 - 池化层 ... 卷积层 - 池化层 - 全连接层 - ... - 全连接层 - 输出层
### 卷积神经网络的动作
定义一个小矩阵，称为卷积核（Convolution Kernal）

将卷积核与输入矩阵作矩阵卷积，将输入矩阵压缩。

卷积核表征了一种我们感兴趣的特征：

与特征匹配的部分，卷积结果是一个较大的数。

但与特征不匹配的部分，卷积结果就可能很小或者为 0。
