# 第五讲-支持向量机
## 支持向量机
点到超平面的距离

`(transform(w) * x1 + b)/abs(w)`

式中 transform() 是转置函数，x1 是标量.

### 支持向量机的最优化目标

取 2&epsilon;/abs(w) 最大，使得 **y·(transform(w)\*x+b)>&beta;**

变形，得

取 (abs(w))^2/2 最大，使得 **y·(transform(w)\*x+b)>&beta;**

以上两式中 &beta;=&epsilon;^2

![GQScn0.png](https://s1.ax1x.com/2020/03/31/GQScn0.png)

## 核函数的选择
![GJmRoT.png](https://s1.ax1x.com/2020/04/02/GJmRoT.png)

![GJmTyR.png](https://s1.ax1x.com/2020/04/02/GJmTyR.png)

如果机器算力不足，需要尽快得到训练模型，那就选用线性核。

如果机器性能足够强，需要更好的性能，可以选择径向基核。

## LIBSVM 工具包的使用
![GJnzNT.png](https://s1.ax1x.com/2020/04/02/GJnzNT.png)

![GJu1KA.png](https://s1.ax1x.com/2020/04/02/GJu1KA.png)

## 线性判别公式
<pre>
    n
y=&Sigma;&alpha;<sub>i</sub>&lt;x<sub>i</sub>,x<sub>i</sub>&gt;+b
    i
</pre>
仍有错误，有待改正。

## 自适应增强（AdaBoost）
### 理论基础
![GJlUwn.png](https://s1.ax1x.com/2020/04/02/GJlUwn.png)

### 操作方法
用许多的弱分类器**自适应加权**，构成一个强分类器。

![GJl6OJ.png](https://s1.ax1x.com/2020/04/02/GJl6OJ.png)

上式表现为各弱分类器的加权求和。

sign() 是符号函数（此处以二分类为例）。
## 具体的 AdaBoost 算法
![GJ1XDJ.png](https://s1.ax1x.com/2020/04/02/GJ1XDJ.png)

上式中
<pre>[y<sub>i</sub>!=h<sub>j</sub>&#40;x<sub>i</sub>&#41;]</pre>
是一个布尔操作，表达式为真时值为1，为假时值为-1
